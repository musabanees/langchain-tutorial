{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ded6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4007b2",
   "metadata": {},
   "source": [
    "## Completion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6e78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8900e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### The Day Tesla Heard Himself in New York\n",
      "\n",
      "In the summer of 1899, when the world was still buzzing with the electric hum of early dynamos, Nikola Tesla was on a mission that would push the boundaries of physics and leave a curious breadcrumb in history.\n",
      "\n",
      "Tesla had built a 100‚Äëfoot ‚Äútower‚Äù in the high desert of Colorado Springs‚Äîan enormous copper‚Äëclad structure with a 20,000‚Äëvolt Tesla coil at its apex. The goal was simple in theory, impossible in practice: transmit electricity without wires over long distances. He wanted to power a city from a single point, a dream of a world where the grid would be invisible.\n",
      "\n",
      "On a clear night, Tesla turned the coil‚Äôs iron core into a roaring furnace of sparks. The air crackled, a faint hiss of ionized air filled the canyon, and a low, almost musical tone resonated through the tower. The experiment was a success‚Äîelectricity flowed from the coil, lighting a nearby lamp that was 40 miles away in Pueblo, Colorado. Tesla was ecstatic; the world was one step closer to wireless power.\n",
      "\n",
      "But then something else happened‚Äîan accidental broadcast that would ripple through the annals of science.\n",
      "\n",
      "Tesla had set up a simple radio receiver‚Äîa crystal radio, a tin can and a coil of wire‚Äîinside the tower‚Äôs basement to monitor the high‚Äëfrequency oscillations. He was listening for stray radio waves, hoping to see if the tower was inadvertently emitting any. The radio crackled, and the receiver began to pick up a faint voice. It was Tesla himself, speaking from the top of the tower, his voice echoing through the canyon.\n",
      "\n",
      "He laughed, a sound that bounced off the canyon walls and reached the receiver. The radio picked up his voice and, to Tesla‚Äôs astonishment, the signal traveled to a radio receiver that had been set up in New York City‚Äôs telephone exchange. The voice of a man in Colorado Springs was heard in Manhattan‚Äîan unintentional, yet unmistakable, wireless transmission of information.\n",
      "\n",
      "Tesla was stunned. He had been focused on the transmission of power, yet the same electromagnetic waves were carrying his voice across the continent. The realization was electrifying: the same physics that allowed electricity to travel without wires could also carry sound, and eventually, any information.\n",
      "\n",
      "He rushed to write to Guglielmo Marconi, the Italian inventor who was racing to patent the first practical radio system. Tesla sent a telegram and a letter, describing his experiment and the accidental transmission of his own voice. He suggested that the wireless transmission of power and communication were not separate endeavors but part of the same grand tapestry of electromagnetism.\n",
      "\n",
      "Marconi, however, was already deep in his own trials and had filed patents on radio transmission. He did not respond to Tesla‚Äôs telegram. Tesla, meanwhile, continued his work, unaware that his accidental broadcast had already prefigured the radio age.\n",
      "\n",
      "Years later, when Marconi‚Äôs patents were challenged, Tesla‚Äôs experiments were cited as evidence that he had, in fact, discovered radio waves long before Marconi. The curious story of the day Tesla heard himself in New York became a footnote in history, a reminder of how a single experiment can yield unexpected, ripple‚Äëlike discoveries.\n",
      "\n",
      "---\n",
      "\n",
      "#### Why This Story Stands Out\n",
      "\n",
      "- **Accidental Innovation**: Tesla was pursuing wireless power, not wireless communication. The broadcast of his voice was a serendipitous side effect.\n",
      "- **Cross‚ÄëContinental Transmission**: The voice traveled from Colorado to New York before commercial radio stations existed, showcasing the power of high‚Äëfrequency waves.\n",
      "- **Precedent for Radio**: This incident foreshadowed the future of radio, showing that Tesla‚Äôs work was not just about power but about the entire electromagnetic spectrum.\n",
      "\n",
      "In the end, Tesla‚Äôs curiosity didn‚Äôt just power a lamp; it lit a path to the wireless world we now live in. And in that moment, when his voice floated across the country, it was a reminder that science is as much about the unexpected as it is about the planned.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {objective} story about {topic}\"\n",
    ")\n",
    "\n",
    "llmmodelprompt = prompt_template.format(\n",
    "    objective=\"curious\",\n",
    "    topic=\"Tesla\"\n",
    ")\n",
    "\n",
    "res = model.invoke(llmmodelprompt)\n",
    "print(res.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ef4bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Provide the Helpful response to the user about the {topic}\"),\n",
    "        (\"user\", \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    topic=\"Country\",\n",
    "    user_input=\"What is the capital of France?\"\n",
    ")\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc69dd",
   "metadata": {},
   "source": [
    "## Fewshots Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84cdbd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Tom bring a ladder to the kitchen?\n",
      "\n",
      "Because he heard the mouse was ‚Äúhigh‚Äëspirited‚Äù and wanted to catch Jerry on a **higher level**! üê±üßó‚Äç‚ôÇÔ∏èüê≠\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "# define the examples\n",
    "examples = [\n",
    "    {\"user\": \"hi!\", \"assistant\": \"¬°hola!\"},\n",
    "    {\"user\": \"bye!\", \"assistant\": \"¬°adi√≥s!\"},\n",
    "]\n",
    "\n",
    "# define the example prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"user\", \"{user}\"),\n",
    "    (\"assistant\", \"{assistant}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=chat_template,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\", \"Provide the helpful response to the user.\"),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_message = final_prompt.format_messages(\n",
    "    user_input=\"Tell me a joke about tom and jerry cartoon.\"\n",
    ")\n",
    "\n",
    "response = model.invoke(input_message)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42a382",
   "metadata": {},
   "source": [
    "Output With Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c63da707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Provide the helpful response to the user.')), FewShotChatMessagePromptTemplate(examples=[{'user': 'hi!', 'assistant': '¬°hola!'}, {'user': 'bye!', 'assistant': '¬°adi√≥s!'}], example_prompt=ChatPromptTemplate(input_variables=['assistant', 'user'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user'], template='{user}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['assistant'], template='{assistant}'))])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa894060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Tom bring a ladder to the kitchen?\n",
      "\n",
      "Because he heard the mouse was ‚Äúon a higher level‚Äù and wanted to **climb** the competition! üê±ü¶†ü§£\n"
     ]
    }
   ],
   "source": [
    "chain = final_prompt | model\n",
    "response = chain.invoke({\"user_input\":\"Tell me a joke about tom and jerry cartoon.\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74a51376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 67,\n",
       "  'prompt_tokens': 124,\n",
       "  'total_tokens': 191,\n",
       "  'completion_time': 0.066314232,\n",
       "  'prompt_time': 0.006164083,\n",
       "  'queue_time': 0.086744162,\n",
       "  'total_time': 0.072478315},\n",
       " 'model_name': 'openai/gpt-oss-20b',\n",
       " 'system_fingerprint': 'fp_77f8660d1d',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a3022bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 124, 'output_tokens': 67, 'total_tokens': 191}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96b2a3",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2bf8fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Russia'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "json_chain = json_prompt | model | json_parser\n",
    "response  = json_chain.invoke({\"question\": \"What is the biggest country?\"})\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07243dab",
   "metadata": {},
   "source": [
    "## Defining the Pydantic Custom Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23073e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Joke( BaseModel ):\n",
    "    setup: str = Field(description=\"Question to make the Joke\")\n",
    "    response: str = Field(descrption=\"Answer to create the Joke\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b3eb6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"Question to make the Joke\", \"type\": \"string\"}, \"response\": {\"title\": \"Response\", \"descrption\": \"Answer to create the Joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"response\"]}\\n```'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054868a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did Tom bring a ladder to the cartoon studio?',\n",
       " 'punchline': 'Because Jerry was always on the high shelf of mischief!'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define a Pydantic Object with the desired output format.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "# Define the parser referring the Pydantic Object\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Add the parser format instructions in the prompt definition.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Create a chain with the prompt and the parser\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": \"Tell me a joke about tom and jerry cartoon.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a81213",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d761fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf70a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a7090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05921a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f97fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030321a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbe58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017bc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c94ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
